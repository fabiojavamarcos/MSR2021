{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv \n",
    "import string\n",
    "import nltk \n",
    "import re \n",
    "import time\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer \n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import sys\n",
    "import warnings\n",
    "from os import path\n",
    "import ast\n",
    "from PIL import Image\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from skmultilearn.problem_transform import BinaryRelevance\n",
    "from skmultilearn.problem_transform import LabelPowerset\n",
    "from skmultilearn.problem_transform import ClassifierChain\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import f1_score, average_precision_score, precision_recall_fscore_support\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import zero_one_loss\n",
    "from sklearn.metrics import jaccard_score\n",
    "import sklearn.metrics as skm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "import numpy as np\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "import time\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn import model_selection\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import average_precision_score\n",
    "from skmultilearn.adapt import MLkNN\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from skmultilearn.adapt import BRkNNaClassifier\n",
    "from sklearn.linear_model import RidgeClassifierCV\n",
    "\n",
    "pd.options.display.max_seq_items = 2000\n",
    "pd.options.display.max_colwidth = 90\n",
    "pd.options.display.max_rows = 999\n",
    "\n",
    "loggingFile = './experiment//TitleBody3GramTFIDF.txt'\n",
    "binaryBodyTitle = './experiment/binaryBodyTitle.csv'\n",
    "template = './experiment/dfTeste.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#organizing data_frame to issue order\n",
    "def organize():\n",
    "    data_classes = pd.read_csv(binaryBodyTitle, header = 0, sep=\";\")\n",
    "    data_classes\n",
    "\n",
    "    del data_classes['prIssue']\n",
    "    del data_classes['issueTitle']\n",
    "    del data_classes['issueBody']\n",
    "\n",
    "    data_classes.rename(columns={'issueComments': 'prComments','Comments': 'prCodeReviewComments','issueTitleLink': 'issueTitle','issueBodyLink': 'issueBody','issueCommentsLink': 'issue_Comments','pr': 'prNumber','issue': 'issueNumber', 'Title': 'prTitle','Body': 'prBody'}, inplace=True)\n",
    "    categories = data_classes.columns.values.tolist()\n",
    "\n",
    "\n",
    "    data_classes = data_classes[['issueNumber', 'prNumber', 'issueTitle', 'issueBody', 'issue_Comments',\n",
    "                            'prTitle','prBody','prComments','prCodeReviewComments','commitMessage','isPR','isTrain',\n",
    "                             'Google Common','Test','SO','IO','UI','Network','Security',\n",
    "                            'OpenOffice Documents','Database','Utils','PDF','Logging','Latex']]\n",
    "\n",
    "    data_classes['issueNumber'] = data_classes['issueNumber'].astype('Int64')\n",
    "    \n",
    "    return data_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_classes = organize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filtering issues with PRs\n",
    "def filtering(data_classes):\n",
    "    IssuePRDataset = data_classes[data_classes[\"isTrain\"] == 0]\n",
    "\n",
    "    #invalid number of issue = NaN\n",
    "    IssuePRDataset = IssuePRDataset.drop([1805])\n",
    "\n",
    "    categories = IssuePRDataset.columns.values.tolist()\n",
    "    \n",
    "    return categories, IssuePRDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#categories, IssuePRDataset = filtering(data_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RQ1.a - o quão sensível o resultado é em relação ao algoritmo? \n",
    "#vários algoritmos - BinaryRelevance\n",
    "#todas as palavras, bootstrap, unigram \n",
    "#somente o título\n",
    "def dataset_config(IssuePRDataset):\n",
    "    data_test1 = IssuePRDataset[['issueNumber','prNumber','issueTitle','issueBody','issue_Comments','Google Common', 'Test', 'SO', 'IO', 'UI', 'Network', 'Security', \n",
    "                           'OpenOffice Documents', 'Database', 'Utils', 'PDF', 'Logging', 'Latex']].copy()\n",
    "\n",
    "    #data_test1[\"corpus\"] = data_test1[\"issueTitle\"].map(str) + ' ' + data_test1[\"issueBody\"].map(str) + ' ' + data_test1[\"issue_Comments\"].map(str)\n",
    "    data_test1[\"corpus\"] = data_test1[\"issueTitle\"].map(str) \n",
    "    del data_test1[\"issueTitle\"]\n",
    "    #del data_test1[\"issueBody\"]\n",
    "    #del data_test1[\"issue_Comments\"]\n",
    "\n",
    "    #removing utils because we won't to predict a so simple API that is basically used in all PRs\n",
    "    del data_test1[\"Utils\"]\n",
    "\n",
    "    data_test1 = data_test1.reset_index(drop=True)\n",
    "    \n",
    "    return data_test1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_test1 = dataset_config(IssuePRDataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocessing text\n",
    "\n",
    "#We first convert the comments to lower-case \n",
    "#then use custom made functions to remove html-tags, punctuation and non-alphabetic characters from the TitleBody.\n",
    "\n",
    "def clean_data(data_test1):\n",
    "    if not sys.warnoptions:\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "\n",
    "    def cleanHtml(sentence):\n",
    "        cleanr = re.compile('<.*?>')\n",
    "        cleantext = re.sub(cleanr, ' ', str(sentence))\n",
    "        return cleantext\n",
    "\n",
    "    def cleanPunc(sentence): #function to clean the word of any punctuation or special characters\n",
    "        cleaned = re.sub(r'[?|!|\\'|\"|#]',r'',sentence)\n",
    "        cleaned = re.sub(r'[.|,|)|(|\\|/]',r' ',cleaned)\n",
    "        cleaned = cleaned.strip()\n",
    "        cleaned = cleaned.replace(\"\\n\",\" \")\n",
    "        return cleaned\n",
    "\n",
    "    def keepAlpha(sentence):\n",
    "        alpha_sent = \"\"\n",
    "        for word in sentence.split():\n",
    "            alpha_word = re.sub('[^a-z A-Z]+', ' ', word)\n",
    "            alpha_sent += alpha_word\n",
    "            alpha_sent += \" \"\n",
    "        alpha_sent = alpha_sent.strip()\n",
    "        return alpha_sent\n",
    "\n",
    "    #function pra remover palavras com menos de 3 tokens\n",
    "\n",
    "    data_test1['corpus'] = data_test1['corpus'].str.lower()\n",
    "    data_test1['corpus'] = data_test1['corpus'].apply(cleanHtml)\n",
    "    data_test1['corpus'] = data_test1['corpus'].apply(cleanPunc)\n",
    "    data_test1['corpus'] = data_test1['corpus'].apply(keepAlpha)\n",
    "    \n",
    "    return data_test1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_test1 = clean_data(data_test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### removing stopwords\n",
    "\n",
    "def remove_stop_words():\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    stop_words.update(['pr','zero','one','two','three','four','five','six','seven','eight','nine','ten','may','also','across','among','beside','however','yet','within','jabref','org','github','com','md','https','ad','changelog','','joelparkerhenderson','localizationupd',' localizationupd','localizationupd ','i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the','Mr', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now'])\n",
    "    #stop_words.update(['i', 'me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\", \"herself\", \"it\", \"its\", \"itself\", \"they\", \"them\", \"their\", \"theirs\", \"themselves\", \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\", \"these\", \"those\", \"am\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \"having\", \"do\", \"does\", \"did\", \"doing\", \"a\", \"an\", \"the\",\"Mr\", \"and\", \"but\", \"if\", \"or\", \"because\", \"as\", \"until\", \"while\", \"of\", \"at\", \"by\", \"for\", \"with\", \"about\", \"against\", \"between\", \"into\", \"through\", \"during\", \"before\", \"after\", \"above\", \"below\", \"to\", \"from\", \"up\", \"down\", \"in\", \"out\", \"on\", \"off\", \"over\", \"under\", \"again\", \"further\", \"then\", \"once\", \"here\", \"there\", \"when\", \"where\", \"why\", \"how\", \"all\", \"any\", \"both\", \"each\", \"few\", \"more\", \"most\", \"other\", \"some\", \"such\", \"no\", \"nor\", \"not\", \"only\", \"own\", \"same\", \"so\", \"than\", \"too\", \"very\", \"s\", \"t\", \"can\", \"will\", \"just\", \"don\", \"should\", \"now\"])\n",
    "\n",
    "    re_stop_words = re.compile(r\"\\b(\" + \"|\".join(stop_words) + \")\\\\W\", re.I)\n",
    "\n",
    "    return re_stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#re_stop_words = remove_stop_words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeStopWords(sentence, re_stop_words):\n",
    "    #global re_stop_words\n",
    "    #print(sentence)\n",
    "    return re_stop_words.sub(\" \", sentence)\n",
    "\n",
    "#removing words with less than 3 characters\n",
    "#data_classes['titleBody'] = data_classes['titleBody'].str.findall('\\w{3,}').str.join(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_test1['corpus'] = data_test1['corpus'].apply(removeStopWords, re_stop_words=re_stop_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_stem(data_test1):\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    \n",
    "    def stemming(sentence):\n",
    "        stemSentence = \"\"\n",
    "        for word in sentence.split():\n",
    "            stem = stemmer.stem(word)\n",
    "            stemSentence += stem\n",
    "            stemSentence += \" \"\n",
    "        stemSentence = stemSentence.strip()\n",
    "        return stemSentence\n",
    "    \n",
    "    data_test1['corpus'] = data_test1['corpus'].apply(stemming)\n",
    "    print(data_test1['corpus'])\n",
    "    \n",
    "    return data_test1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_test1 = apply_stem(data_test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TF-ID\n",
    "def run_tf_idf(data, configurationTFIDF, num_feature, tfIDFoutputFile):\n",
    "    #we need to text max_feature with 10, 20, 25, 50 \n",
    "    #, max_features=num_feature\n",
    "    vectorizer = TfidfVectorizer(strip_accents='unicode', analyzer='word', ngram_range = configurationTFIDF, max_features=num_feature)\n",
    "        \n",
    "    tf_idf_results = vectorizer.fit_transform(data['corpus'])\n",
    "\n",
    "    features = vectorizer.get_feature_names()\n",
    "\n",
    "    print(features)\n",
    "\n",
    "    scores = (tf_idf_results.toarray())\n",
    "    output_tf_idf = pd.DataFrame(scores)\n",
    "    \n",
    "    output_tf_idf = pd.concat([data['issueNumber'], output_tf_idf], axis=1)\n",
    "\n",
    "    output_tf_idf.to_csv(tfIDFoutputFile, encoding='utf-8', header=False, index=False, sep=',')\n",
    "\n",
    "\n",
    "    # remove words occuring less than 5 times\n",
    "    #tfidf = TfidfVectorizer(min_df=5)\n",
    "    #you can also remove common words:\n",
    "\n",
    "    # remove words occuring in more than half the documents\n",
    "    #tfidf = TfidfVectorizer(max_df=0.5)\n",
    "    #you can also remove stopwords like this:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use one of the data_test's (1 to 11) e.g data_test5\n",
    "\n",
    "#run_tf_idf(data_test1, configurationTFIDF, num_feature, tfIDFoutputFile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#analyzing frequency of TOP 50 terms\n",
    "\n",
    "def analyze_top(data, termFrequencyTop50):\n",
    "    docs = data['corpus'].tolist()\n",
    "\n",
    "    cv = CountVectorizer()\n",
    "    cv_fit=cv.fit_transform(docs)\n",
    "\n",
    "    #print(cv.get_feature_names())\n",
    "    #print(cv_fit.toarray())\n",
    "\n",
    "    word_list = cv.get_feature_names()   \n",
    "\n",
    "    count_list = cv_fit.toarray().sum(axis=0)\n",
    "    term_frequency = dict(zip(word_list,count_list))\n",
    "\n",
    "    a = sorted(term_frequency.items(), key=lambda x: x[1], reverse=True) \n",
    "\n",
    "    print('SIZE OF TERMS', len(a))\n",
    "\n",
    "    top50 = a[:100]\n",
    "    df_frequency = pd.DataFrame(top50, columns =['term', 'frequency'])  \n",
    "\n",
    "    print(df_frequency)\n",
    "\n",
    "    df_frequency.to_csv(termFrequencyTop50, encoding='utf-8', header=False, index=False, sep=',')\n",
    "\n",
    "    sns.set(font_scale = 2)\n",
    "    plt.figure(figsize=(18,17))\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.title(\"Frequency of terms \")\n",
    "    plt.ylabel('term', fontsize=20)\n",
    "    plt.xlabel('frequency', fontsize=20)\n",
    "    ax = sns.barplot(x=\"frequency\", y=\"term\", data=df_frequency)\n",
    "    \n",
    "    return docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#docs = analyze_top(data_test1, termFrequencyTop50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merging features TF-IDF with data_frame\n",
    "def merging(data_test1, tfIDFoutputFile):\n",
    "    feature = pd.read_csv(tfIDFoutputFile, header=None, sep=\",\")\n",
    "    feature.rename(columns={0: 'issueNumber'}, inplace=True)\n",
    "\n",
    "    data_classifier = data_test1.join(feature, lsuffix='issueNumber', rsuffix='issueNumber')\n",
    "\n",
    "    categories = data_classifier.columns.values.tolist()\n",
    "    \n",
    "    return data_classifier, categories\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_classifier, categories = merging(data_test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_metrics(predictions, probabilities, y_test):\n",
    "    \n",
    "    y_pred = predictions.values\n",
    "    y_proba = probabilities.values\n",
    "\n",
    "    #receiving the y_test true value from each pull request\n",
    "    y_true = y_test.to_numpy()\n",
    "\n",
    "    print(\"Accuracy Score\")\n",
    "    acc_ml = accuracy_score(y_true, y_pred)\n",
    "    print(acc_ml)\n",
    "\n",
    "    print(\"---------\")\n",
    "    print(\"\")\n",
    "\n",
    "    print(\"Accuracy Score not normalized\")\n",
    "    acc_score = accuracy_score(y_true, y_pred, normalize=False)\n",
    "    print(acc_score)\n",
    "\n",
    "    print(\"---------\")\n",
    "    print(\"\")\n",
    "\n",
    "    print(\"zero_one_loss\")\n",
    "    zeroOne = zero_one_loss(y_true, y_pred)\n",
    "    print(zeroOne)\n",
    "\n",
    "    print(\"---------\")\n",
    "    print(\"\")\n",
    "\n",
    "    print(\"Fmeasure Score\")\n",
    "    fmeasure_score = f1_score(y_true,y_pred, average='micro')\n",
    "    #fmeasure_score = f1_score(y_true,y_pred, average='macro')\n",
    "    print(fmeasure_score)\n",
    "\n",
    "    print(\"---------\")\n",
    "    print(\"\")\n",
    "\n",
    "    #AUC-PR\n",
    "    print(\"AUC-PR\")\n",
    "    pr_score = average_precision_score(y_true,y_proba,average='micro')\n",
    "    print(pr_score)\n",
    "    #pr_score = average_precision_score(y_true,y_proba,average='macro')\n",
    "\n",
    "    print(\"---------\")\n",
    "    print(\"\")\n",
    "\n",
    "\n",
    "    print(\"hamming loss average\")\n",
    "    hamming_loss = skm.hamming_loss(y_true, y_pred)\n",
    "    print(hamming_loss)\n",
    "\n",
    "    print(\"---------\")\n",
    "    print(\"\")\n",
    "\n",
    "    print(\"Jaccard samples\")\n",
    "    jaccard_score_samples = jaccard_score(y_true, y_pred, average='samples')\n",
    "    print(jaccard_score_samples)\n",
    "\n",
    "    print(\"---------\")\n",
    "    print(\"\")\n",
    "\n",
    "\n",
    "    print(\"Jaccard macro\")\n",
    "    jaccard_macro = jaccard_score(y_true, y_pred, average='micro')\n",
    "    print(jaccard_macro)\n",
    "    #jaccard_score(y_true, y_pred, average=None)\n",
    "\n",
    "    print(\"---------\")\n",
    "    print(\"\")\n",
    "\n",
    "    return y_true, y_proba, y_pred, acc_ml, acc_score, zeroOne, fmeasure_score, pr_score, hamming_loss, jaccard_score_samples, jaccard_macro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_classes(probability, y_true, y_test):\n",
    "    precision = dict()\n",
    "    recall = dict()\n",
    "    average_precision = dict()\n",
    "\n",
    "    n_classes = y_test.shape[1]\n",
    "\n",
    "    for i in range(n_classes):\n",
    "        precision[i], recall[i], _ = precision_recall_curve(y_true[:, i], probability[:, i])\n",
    "        average_precision[i] = average_precision_score(y_true[:, i], probability[:, i])\n",
    "\n",
    "    precision[\"micro\"], recall[\"micro\"], _ = precision_recall_curve(y_true.ravel(),probability.ravel())\n",
    "\n",
    "    average_precision[\"micro\"] = average_precision_score(y_true, probability, average=\"micro\")\n",
    "\n",
    "    print('Average precision score, micro-averaged over all classes: {0:0.2f}'\n",
    "          .format(average_precision[\"micro\"]))\n",
    "\n",
    "    plt.figure()\n",
    "    plt.step(recall['micro'], precision['micro'], where='post')\n",
    "\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.title(\n",
    "        'Average precision score, micro-averaged over all classes: AP={0:0.2f}'\n",
    "        .format(average_precision[\"micro\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "def confusion_matrix(y_true, y_pred, confusionMatrix, i):\n",
    "\n",
    "    data = multilabel_confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    labels = ['Google Common', 'Test', 'SO', 'IO', 'UI', 'Network',\n",
    "              'Security', 'OpenOffice Documents', 'Database','PDF',\n",
    "              'Logging','Latex']\n",
    "      \n",
    "    metrics = pd.DataFrame()\n",
    "    line = []\n",
    "    dataLine = \"\"\n",
    "    dataLine = \"Label, TN, FP, FN, TP\"\n",
    "    line.append((dataLine))\n",
    "\n",
    "    for j in range (0,12):\n",
    "        print(j)\n",
    "        row = data[j]\n",
    "        dataLine=\"\"\n",
    "        dataLine = labels[j] \n",
    "        print(dataLine)\n",
    "        for x in np.nditer(row):\n",
    "            dataLine = dataLine + \",\" + str(x)\n",
    "            print(dataLine)\n",
    "        line.append((dataLine))\n",
    "        print(line)\n",
    "        \n",
    "        metrics = pd.DataFrame(line)\n",
    "    \n",
    "    metrics.to_csv(confusionMatrix + str(i) +'.csv' , encoding='utf-8', header=True, index=False , sep=',')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build the model \n",
    "\n",
    "def build_model(test_type):\n",
    "\n",
    "    if test_type == \"DecisionTree\":\n",
    "        clf = BinaryRelevance(classifier=DecisionTreeClassifier(), require_dense = [False, True])\n",
    "        #clf = ClassifierChain(classifier=DecisionTreeClassifier(), require_dense = [False, True])\n",
    "    if test_type == \"LogisticRegression\":\n",
    "        clf = BinaryRelevance(classifier=LogisticRegression(random_state=0), require_dense = [False, True])\n",
    "        #clf = ClassifierChain(classifier=LogisticRegression(random_state=0), require_dense = [False, True])\n",
    "    if test_type == \"RandomForest\": \n",
    "        clf = BinaryRelevance(classifier=RandomForestClassifier(criterion='entropy',max_depth= 50, min_samples_leaf= 1, min_samples_split= 3, n_estimators= 50), require_dense = [False, True])\n",
    "        #clf = ClassifierChain(classifier=RandomForestClassifier(criterion='entropy',max_depth= 50, min_samples_leaf= 1, min_samples_split= 3, n_estimators= 50), require_dense = [False, True])\n",
    "    if test_type == \"MLPClassifier\":\n",
    "        clf = BinaryRelevance(classifier=MLPClassifier(hidden_layer_sizes=(150,100,50), max_iter=300,activation = 'relu',solver='adam',random_state=1), require_dense = [False, True])\n",
    "        #clf = ClassifierChain(classifier=MLPClassifier(hidden_layer_sizes=(150,100,50), max_iter=300,activation = 'relu',solver='adam',random_state=1), require_dense = [False, True])\n",
    "    if test_type == \"MLkNN\":\n",
    "        clf = BinaryRelevance(MLkNN(k=3))\n",
    "        #clf = ClassifierChain(classifier=MLkNN(k=3))\n",
    "\n",
    "    #This three works without probability\n",
    "    if test_type == \"LinearSVC\":\n",
    "        clf = BinaryRelevance(classifier=LinearSVC(), require_dense = [False, True])\n",
    "    if test_type == \"GaussianNB\":\n",
    "        clf = BinaryRelevance(classifier=GaussianNB(), require_dense = [False, True])\n",
    "    if test_type == \"RidgeClassifierCV\":\n",
    "        clf = BinaryRelevance(classifier=RidgeClassifierCV(alphas=[1e-3, 1e-2, 1e-1, 1]), require_dense = [False, True]) \n",
    "    if test_type == \"BRkNNaClassifier\":\n",
    "        clf = BinaryRelevance(BRkNNaClassifier(k=3))\n",
    "\n",
    "        \n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_metrics(y_true, y_pred, acc_ml,acc_score,zeroOne,pr_score,hamming_loss,jaccard_score_samples,jaccard_macro, modelMatrix, metrics_by_class, i, configurationTFIDF ,num_feature ,stop_word ,size_test,test_type):\n",
    "\n",
    "    line=[] \n",
    "    # line to csv report file\n",
    "    \n",
    "    names = ['Google Common',\n",
    "'Test',\n",
    "'SO',\n",
    "'IO',\n",
    "'UI',\n",
    "'Network',\n",
    "'Security',\n",
    "'OpenOffice Documents',\n",
    "'Database',\n",
    "'PDF',\n",
    "'Logging',\n",
    "'Latex']\n",
    "    \n",
    "\n",
    "    prec, rec, fscore, sup = precision_recall_fscore_support(y_true,y_pred, average='micro')\n",
    "\n",
    "\n",
    "    arr = [acc_ml,acc_score,zeroOne,pr_score,hamming_loss,jaccard_score_samples,jaccard_macro,prec,rec, fscore]\n",
    "    columns = ['Accuracy','Acc-Score','zero_one_loss','AUC-PR','hamming loss average','Jaccard samples','Jaccard macro','Precision','Recall','Fmeasure']\n",
    "  \n",
    "    df_metrics2 = pd.DataFrame([arr],columns=columns)\n",
    "    print(df_metrics2)\n",
    "\n",
    "    print(\"---------\")\n",
    "    print(\"\")\n",
    "\n",
    "    x = precision_recall_fscore_support(y_true,y_pred, average=None)\n",
    "    print(\"\")\n",
    "    print(\"\")\n",
    "\n",
    "    df_metrics_by_class = pd.DataFrame.from_records(x, columns=names, index=['precision','recall','f-measure','samples_tested'])\n",
    "    print(df_metrics_by_class)\n",
    "\n",
    "    print(\"---------\")\n",
    "    print(\"\")\n",
    "\n",
    "    df_metrics2.to_csv(modelMatrix, encoding='utf-8', header=True, index=False, sep=',')    \n",
    "\n",
    "    df_metrics_by_class.to_csv(metrics_by_class, encoding='utf-8', header=True, index=False, sep=',')    \n",
    "\n",
    "    dataLine = \"\"\n",
    "    dataLine = \"tf-IDFMin, tf-IDFMax, #_TopTerms,Stop_Word,Train/Test_Size,Algorithm,Accuracy_Score,Accuracy_Score_not_normalized,zero_one_loss, AUC-PR,hamming_loss_avg,Jaccard_samples,Jaccard_macro,Precision,Recall,Fmeasure_Score, i\"\n",
    "    line.append((dataLine))\n",
    "    dataLine =  str(configurationTFIDF) + \",\" + str(num_feature) + \",\" + stop_word + \",\" + str(size_test) + \",\" + str(test_type) + \",\" + str(acc_ml) + \",\"+ str(acc_score) + \",\"+ str(zeroOne) + \",\" + str(pr_score) + \",\"+ str(hamming_loss) + \",\"+ str(jaccard_score_samples) + \",\"+ str(jaccard_macro)+\",\" + str(prec)+\",\"+str(rec)+\",\"+str(fscore)+\",\"+str(i) \n",
    "    line.append((dataLine))\n",
    "    print(line)\n",
    "\n",
    "    metrics = pd.DataFrame(line)\n",
    "    metrics.to_csv('./experiment/report'+ str(configurationTFIDF) + str(num_feature) + stop_word + str(size_test)+ str(test_type)+str(i)+'.csv', encoding='utf-8', header=False, index=False, sep=',')    \n",
    "    np.savetxt(r'./experiment/report'+ str(configurationTFIDF) + str(num_feature) + stop_word + str(size_test)+ str(test_type)+str(i)+'.txt', metrics.values, fmt='%s', delimiter=',')\n",
    "\n",
    "    return prec, rec, fscore, sup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "def persist_data(configurationTFIDF ,num_feature , stop_word , size_test , test_type , acc_ml , \n",
    "           acc_score , zeroOne , pr_score, hamming_loss, jaccard_score_samples, \n",
    "           jaccard_macro , prec , rec, fscore, i):\n",
    "    now = datetime.datetime.now()\n",
    "\n",
    "    templateData = pd.read_csv(template, sep=',')\n",
    "    \n",
    "    print (\"Current date and time : \")\n",
    "    print (now.strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "    headerdf = ['date_time','tf-IDF', '#_TopTerms','Stop_Word','Train/Test_Size','Algorithm','Accuracy_Score',\n",
    "                'Accuracy_Score_not_normalized','zero_one_loss', 'AUC-PR','hamming_loss_avg','Jaccard_samples',\n",
    "                'Jaccard_macro','Precision','Recall','Fmeasure_Score','i']\n",
    "\n",
    "\n",
    "    tup = ( now,str(configurationTFIDF) ,str(num_feature) , stop_word , str(size_test) , str(test_type) , str(acc_ml) , \n",
    "           str(acc_score) , str(zeroOne) , str(pr_score) , str(hamming_loss) , str(jaccard_score_samples) , \n",
    "           str(jaccard_macro) , str(prec) , str(rec) , str(fscore) ,str(i))\n",
    "\n",
    "    print(\"tupla:\", tup)\n",
    "    print(len(tup))\n",
    "\n",
    "    list_tup  = [ now,str(configurationTFIDF) ,str(num_feature) , stop_word , str(size_test) , str(test_type) , str(acc_ml) , \n",
    "           str(acc_score) , str(zeroOne) , str(pr_score) , str(hamming_loss) , str(jaccard_score_samples) , \n",
    "           str(jaccard_macro) , str(prec) , str(rec) , str(fscore) ,str(i)]\n",
    "\n",
    "\n",
    "    dfTeste = pd.DataFrame.from_records(data=[tup], columns=[headerdf])\n",
    "\n",
    "    data_list = templateData.values.tolist()\n",
    "    data_list.append(list_tup)\n",
    "    new_data = pd.DataFrame(data_list)  \n",
    "    for row in templateData.itertuples():\n",
    "        print(row)\n",
    "\n",
    "    new_data.to_csv('./experiment/dfTeste.csv', encoding='utf-8', index=False, sep=',', header=headerdf)  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_split(data_classifier, test_type, confusionMatrix, modelMatrix, metrics_by_class, configurationTFIDF ,num_feature ,stop_word ,size_test):\n",
    "    train = []\n",
    "    test = []\n",
    "\n",
    "    X = data_classifier\n",
    "\n",
    "    rs = ShuffleSplit(n_splits=splits, test_size= size_test, random_state=52)\n",
    "    rs.get_n_splits(X)\n",
    "\n",
    "    for train_index, test_index in rs.split(X):\n",
    "         #print(\"%s %s\" % (train_index, test_index))\n",
    "         train.append(train_index)\n",
    "         test.append(test_index)\n",
    "        \n",
    "    for i in range(0, len(train)):\n",
    " \n",
    "        size_features = len(X.columns)\n",
    "        data = data_classifier.ix[train[i]]\n",
    "        X_train = data.iloc[:,15:size_features]\n",
    "        #del X_train['issueNumberissueNumber']\n",
    "        y_train = data.loc[:,'Google Common':'Latex']\n",
    "\n",
    "        data = data_classifier.ix[test[i]]\n",
    "        X_test = data.iloc[:,15:size_features]\n",
    "        y_test = data.loc[:,'Google Common':'Latex']\n",
    "\n",
    "        categories = y_test.columns.values.tolist()\n",
    "        ids = y_test.index\n",
    "\n",
    "        classifier_setup = build_model(test_type)\n",
    "    \n",
    "        clf = classifier_setup\n",
    "        clf.fit(X_train,y_train)\n",
    "\n",
    "        predict = clf.predict(X_test).toarray()\n",
    "        probability = clf.predict_proba(X_test).toarray()\n",
    "\n",
    "        predictions = pd.DataFrame(predict, index=ids, columns=categories) # with header\n",
    "        probabilities = pd.DataFrame(probability, index=ids, columns=categories) # with header\n",
    "\n",
    "        y_pred = predictions.values\n",
    "        y_proba = probabilities.values\n",
    "\n",
    "        y_true = y_test.to_numpy()\n",
    "\n",
    "        y_true, y_proba, y_pred, acc_ml, acc_score, zeroOne, fmeasure_score, pr_score, hamming_loss, jaccard_score_samples, jaccard_macro = eval_metrics(predictions, probabilities, y_test)\n",
    "\n",
    "        plot_classes(probability, y_true, y_test)\n",
    "\n",
    "        confusion_matrix(y_true, y_pred, confusionMatrix, i)\n",
    "        \n",
    "        prec, rec, fscore, sup = save_metrics(y_true, y_pred, acc_ml,acc_score,zeroOne,pr_score,hamming_loss,jaccard_score_samples,jaccard_macro, modelMatrix, metrics_by_class, i, configurationTFIDF ,num_feature ,stop_word ,size_test,test_type)\n",
    "\n",
    "        persist_data(configurationTFIDF ,num_feature , stop_word , size_test , test_type , acc_ml , \n",
    "           acc_score , zeroOne , pr_score, hamming_loss, jaccard_score_samples, \n",
    "           jaccard_macro , prec , rec, fscore, i)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of shuffles (folds)\n",
    "splits=10\n",
    "\n",
    "#defining paths\n",
    "loggingFile = './experiment//TitleBody3GramTFIDF.txt'\n",
    "binaryBodyTitle = './experiment/binaryBodyTitle.csv'\n",
    "template = './experiment/dfTeste.csv'\n",
    "\n",
    "configurationTFIDFList = [(1,1)]\n",
    "#configurationTFIDFList = [(1,1),(2,2),(3,3),(4,4)]\n",
    "#num_featureList = [25,50,100,500,1000,2500,5000]\n",
    "num_featureList = [11456]\n",
    "size_testList = [0.2]\n",
    "#size_testList = [0.2,0.3,0.4]\n",
    "stop_wordList = [\"Yes\"]\n",
    "#test_typeList = [\"RandomForest\",\"DecisionTree\",\"LogisticRegression\",\"MLPClassifier\",\"MLkNN\"]\n",
    "test_typeList = [\"RandomForest\"]\n",
    "\n",
    "#examples\n",
    "#configurationTFIDFList = [(1,1),(2,2)]\n",
    "#num_featureList = [25,50]\n",
    "#size_testList = [0.2,0.3]\n",
    "#stop_wordList = [\"Yes\",\"No\"]\n",
    "#test_typeList = [\"RandomForest\",\"DecisionTree\"]\n",
    "\n",
    "configurationTFIDF=(1,1)\n",
    "num_feature=25\n",
    "\n",
    "size_test=0.2\n",
    "\n",
    "#stop_word = stop_wordList[i]\n",
    "stop_word = \"Yes\"\n",
    "\n",
    "test_type = \"RandomForest\"\n",
    "# dont forget to have the file dfTeste.csv ready in the files folder only with the header!\n",
    "\n",
    "def __main__():\n",
    "\n",
    "    # getting length of list \n",
    "    lengthT = len(configurationTFIDFList) \n",
    "    lengthF = len(num_featureList) \n",
    "    lengthS = len(size_testList)\n",
    "    lengthY = len(test_typeList)\n",
    "    \n",
    "    # Iterating the index \n",
    "    # same as 'for i in range(len(list))' \n",
    "    for t in range(lengthT): \n",
    "        for f in range(lengthF): \n",
    "            for s in range(lengthS):\n",
    "                for y in range(lengthY):\n",
    "                    print(\"----------------\") \n",
    "                    print(configurationTFIDFList[t]) \n",
    "                    print(num_featureList[f]) \n",
    "                    print(size_testList[s]) \n",
    "                    print(test_typeList[y]) \n",
    "                    print(\"----------------\") \n",
    "\n",
    "                    configurationTFIDF=configurationTFIDFList[t]\n",
    "                    num_feature=num_featureList[f]\n",
    "\n",
    "                    size_test=size_testList[s]\n",
    "\n",
    "                    #stop_word = stop_wordList[i]\n",
    "                    stop_word = \"Yes\"\n",
    "\n",
    "                    test_type = test_typeList[y]\n",
    "                    \n",
    "                    tfIDFoutputFile = './experiment/tfIDFoutputFile'+ str(configurationTFIDF) + str(num_feature) + stop_word + str(size_test)+ str(test_type)+'.csv'\n",
    "                    classifierFeatureInput='./experiment/train_file_test'+ str(configurationTFIDF) + str(num_feature) + stop_word + str(size_test)+ str(test_type)+'.csv'\n",
    "                    termFrequencyTop50 = './experiment/termFrequencyTop50'+ str(configurationTFIDF) + str(num_feature) + stop_word + str(size_test)+ str(test_type)+'.csv'\n",
    "                    predictions_result = './experiment/predict_file_'+ str(configurationTFIDF) + str(num_feature) + stop_word + str(size_test)+ str(test_type)+'.csv'\n",
    "                    probabilities_result = './experiment/probability_file_'+ str(configurationTFIDF) + str(num_feature) + stop_word + str(size_test)+ str(test_type)+'.csv'\n",
    "                    modelMatrix = './experiment/modelMatrix'+ str(configurationTFIDF) + str(num_feature) + stop_word + str(size_test)+ str(test_type)+'.csv'\n",
    "                    metrics_by_class = './experiment/metrics_By_Class'+ str(configurationTFIDF) + str(num_feature) + stop_word + str(size_test)+ str(test_type)+'.csv'  \n",
    "                    confusionMatrix = './experiment/CM'+ str(configurationTFIDF) + str(num_feature) + stop_word + str(size_test)+ str(test_type)\n",
    "                    \n",
    "\n",
    "                    data_classes = organize()\n",
    "                    categories, IssuePRDataset = filtering(data_classes)\n",
    "                    data_test1 = dataset_config(IssuePRDataset)\n",
    "                    data_test1 = clean_data(data_test1)\n",
    "                    \n",
    "                    re_stop_words = remove_stop_words()\n",
    "                    data_test1['corpus'] = data_test1['corpus'].apply(removeStopWords, re_stop_words=re_stop_words)\n",
    "                    data = data_test1\n",
    "                    data_test1 = apply_stem(data)\n",
    "                    run_tf_idf(data_test1, configurationTFIDF, num_feature, tfIDFoutputFile)\n",
    "                    \n",
    "                    docs = analyze_top(data_test1, termFrequencyTop50)\n",
    "                    data_classifier, categories = merging(data_test1, tfIDFoutputFile)\n",
    "                    run_split(data_classifier, test_type, confusionMatrix, modelMatrix, metrics_by_class, configurationTFIDF ,num_feature ,stop_word ,size_test)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "__main__()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
