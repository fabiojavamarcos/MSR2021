{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "#https://altair-viz.github.io/gallery/errorbars_with_std.html\n",
    "import altair as alt\n",
    "from vega_datasets import data\n",
    "\n",
    "#defining paths\n",
    "allAlgorithms_unigram_900terms = './boxplotH5.csv'\n",
    "#classifierChain = './experiment/dfTesteClassifierChain_13Labels.csv'\n",
    "\n",
    "\n",
    "dataBinary = pd.read_csv(allAlgorithms_unigram_900terms)\n",
    "#dataClassifier = pd.read_csv(classifierChain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataBinary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd=pd.melt(dataBinary,id_vars=['Hypothesis'],value_vars=['Precision','Recall','Fmeasure_Score'],var_name='Evaluation Metrics')\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "ax = sns.boxplot(y='value',x='Hypothesis',data=dd,hue='Evaluation Metrics')\n",
    "plt.ylabel(\"Performance\", size=12)\n",
    "plt.xlabel(\"Evaluation Metrics by Hypothesis\",size=12)\n",
    "\n",
    "labels=[\"Precision\", \"Recall\", \"F-measure\"]\n",
    "h, l = ax.get_legend_handles_labels()\n",
    "ax.legend(h, labels, title=\"Evaluation Metrics\",bbox_to_anchor=(1.02, 1), loc=2, borderaxespad=0.)\n",
    "plt.title(\"Evaluation Metrics by Hypothesis\", size=12)\n",
    "plt.show()\n",
    "#plt.savefig(\"grouped_boxplot_AlgorithmBinary.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataBinary.groupby(['Hypothesis']).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataBinary.nlargest(5,['Precision','Fmeasure_Score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "ax = sns.boxplot(y='AUC-PR',x='Hypothesis',data=dataBinary)\n",
    "plt.ylabel(\"AUCpr distribution\", size=12)\n",
    "plt.xlabel(\"Hypothesis\",size=12)\n",
    "plt.title(\"Classifier Chain - AUCpr Analysis\", size=12)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "ax = sns.boxplot(y='hamming_loss_avg',x='Hypothesis',data=dataBinary)\n",
    "plt.ylabel(\"Hamming Loss Average distribution\", size=12)\n",
    "plt.xlabel(\"Hypothesis\",size=12)\n",
    "plt.title(\"Classifier Chain - Hamming Loss Analysis\", size=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "ax = sns.boxplot(y='Accuracy_Score_not_normalized',x='Hypothesis',data=dataBinary)\n",
    "plt.ylabel(\"PR's correctly predicted\", size=12)\n",
    "plt.xlabel(\"Algorithms\",size=12)\n",
    "plt.title(\"Classifier Chain - Number of PR correctly predict considering all APIs\", size=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cliffsDelta(lst1, lst2, **dull):\n",
    "\n",
    "    \"\"\"Returns delta and true if there are more than 'dull' differences\"\"\"\n",
    "    if not dull:\n",
    "        dull = {'small': 0.147, 'medium': 0.33, 'large': 0.474} # effect sizes from (Hess and Kromrey, 2004)\n",
    "    m, n = len(lst1), len(lst2)\n",
    "    lst2 = sorted(lst2)\n",
    "    j = more = less = 0\n",
    "    for repeats, x in runs(sorted(lst1)):\n",
    "        while j <= (n - 1) and lst2[j] < x:\n",
    "            j += 1\n",
    "        more += j*repeats\n",
    "        while j <= (n - 1) and lst2[j] == x:\n",
    "            j += 1\n",
    "        less += (n - j)*repeats\n",
    "    d = (more - less) / (m*n)\n",
    "    size = lookup_size(d, dull)\n",
    "    return d, size\n",
    "\n",
    "\n",
    "def lookup_size(delta: float, dull: dict) -> str:\n",
    "    \"\"\"\n",
    "    :type delta: float\n",
    "    :type dull: dict, a dictionary of small, medium, large thresholds.\n",
    "    \"\"\"\n",
    "    delta = abs(delta)\n",
    "    if delta < dull['small']:\n",
    "        return 'negligible'\n",
    "    if dull['small'] <= delta < dull['medium']:\n",
    "        return 'small'\n",
    "    if dull['medium'] <= delta < dull['large']:\n",
    "        return 'medium'\n",
    "    if delta >= dull['large']:\n",
    "        return 'large'\n",
    "\n",
    "\n",
    "def runs(lst):\n",
    "    \"\"\"Iterator, chunks repeated values\"\"\"\n",
    "    for j, two in enumerate(lst):\n",
    "        if j == 0:\n",
    "            one, i = two, 0\n",
    "        if one != two:\n",
    "            yield j - i, one\n",
    "            i = j\n",
    "        one = two\n",
    "    yield j - i + 1, two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://machinelearningmastery.com/effect-size-measures-in-python/\n",
    "#Small Effect Size: d=0.20\n",
    "#Medium Effect Size: d=0.50\n",
    "#Large Effect Size: d=0.80\n",
    "\n",
    "from numpy import mean\n",
    "from numpy import var\n",
    "from math import sqrt\n",
    "\n",
    "def cohend(d1, d2):\n",
    "\t# calculate the size of samples\n",
    "\tn1, n2 = len(d1), len(d2)\n",
    "\t# calculate the variance of the samples\n",
    "\ts1, s2 = var(d1, ddof=1), var(d2, ddof=1)\n",
    "\t# calculate the pooled standard deviation\n",
    "\ts = sqrt(((n1 - 1) * s1 + (n2 - 1) * s2) / (n1 + n2 - 2))\n",
    "\t# calculate the means of the samples\n",
    "\tu1, u2 = mean(d1), mean(d2)\n",
    "\t# calculate the effect size\n",
    "\treturn (u1 - u2) / s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Subsetting data to compute Stats\n",
    "\n",
    "Baseline_binary = dataBinary[dataBinary.Hypothesis == 'Baseline']\n",
    "Baseline_FMeasure = Baseline_binary['Fmeasure_Score']\n",
    "\n",
    "H5_binary = dataBinary[dataBinary.Hypothesis == 'H5']\n",
    "H5_FMeasure = H5_binary['Fmeasure_Score']\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Kruskall Wallis - Group Comparison\n",
    "\n",
    "#Fail to Reject H0: Paired sample distributions are equal.\n",
    "#Reject H0: Paired sample distributions are not equal.\n",
    "    \n",
    "from scipy.stats import kruskal\n",
    "# seed the random number generator\n",
    "\n",
    "\n",
    "# compare samples\n",
    "stat, p = kruskal(Baseline_FMeasure,H5_FMeasure)\n",
    "print('Statistics=%.3f, p=%.3f' % (stat, p))\n",
    "# interpret\n",
    "alpha = 0.05\n",
    "if p > alpha:\n",
    "\tprint('Same distributions (fail to reject H0)')\n",
    "else:\n",
    "\tprint('Different distributions (reject H0)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://machinelearningmastery.com/nonparametric-statistical-significance-tests-in-python/\n",
    "from scipy.stats import friedmanchisquare\n",
    "\n",
    "# compare samples\n",
    "stat, p = friedmanchisquare(Baseline_FMeasure,H5_FMeasure)\n",
    "print('Statistics=%.3f, p=%.3f' % (stat, p))\n",
    "# interpret\n",
    "alpha = 0.05\n",
    "if p > alpha:\n",
    "\tprint('Same distributions (fail to reject H0)')\n",
    "else:\n",
    "\tprint('Different distributions (reject H0)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### MANN-U Independent Samples\n",
    "from scipy.stats import mannwhitneyu\n",
    "\n",
    "#Fail to Reject H0: Sample distributions are equal.\n",
    "#Reject H0: Sample distributions are not equal.\n",
    "\n",
    "##### Defining variables to be comparede\n",
    "\n",
    "# compare samples\n",
    "stat, p = mannwhitneyu(Baseline_FMeasure,H5_FMeasure)\n",
    "print('Statistics=%.3f, p=%.3f' % (stat, p))\n",
    "# interpret\n",
    "alpha = 0.05\n",
    "if p > alpha:\n",
    "\tprint('Same distribution (fail to reject H0)')\n",
    "else:\n",
    "\tprint('Different distribution (reject H0)')\n",
    "\n",
    "   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Effect_Size RandomForest Vs others   \n",
    "print(cliffsDelta(Baseline_FMeasure, H5_FMeasure))\n",
    "#print(cliffsDelta(RF_FMeasure, MLPC_FMeasure))\n",
    "#print(cliffsDelta(RF_FMeasure, DT_FMeasure))\n",
    "#print(cliffsDelta(RF_FMeasure, MlkNN_FMeasure)) #divisionByZero error\n",
    "\n",
    "#Effect_Size LogisticRegression Vs others\n",
    "#print(cliffsDelta(LogisticRegression_FMeasure, MLPC_FMeasure))\n",
    "#print(cliffsDelta(LogisticRegression_FMeasure, DT_FMeasure))\n",
    "#print(cliffsDelta(LogisticRegression_FMeasure, MlkNN_FMeasure))\n",
    "\n",
    "#Effect_Size MLPC Vs others\n",
    "#print(cliffsDelta(MLPC_FMeasure, DT_FMeasure))\n",
    "#print(cliffsDelta(MLPC_FMeasure, MlkNN_FMeasure))\n",
    "\n",
    "#Effect_Size MlkNN Vs others\n",
    "#print(cliffsDelta(MlkNN_FMeasure,DT_FMeasure))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Effect_Size RandomForest Vs others   \n",
    "print('Cohens d: %.3f' % cohend(RF_FMeasure, LogisticRegression_FMeasure))    \n",
    "print('Cohens d: %.3f' % cohend(RF_FMeasure, MLPC_FMeasure))\n",
    "print('Cohens d: %.3f' % cohend(RF_FMeasure, DT_FMeasure))\n",
    "print('Cohens d: %.3f' % cohend(RF_FMeasure, MlkNN_FMeasure))\n",
    "\n",
    "#Effect_Size LogisticRegression Vs others\n",
    "print('Cohens d: %.3f' % cohend(LogisticRegression_FMeasure, MLPC_FMeasure))    \n",
    "print('Cohens d: %.3f' % cohend(LogisticRegression_FMeasure, DT_FMeasure))    \n",
    "print('Cohens d: %.3f' % cohend(LogisticRegression_FMeasure, MlkNN_FMeasure))    \n",
    "\n",
    "#Effect_Size MLPC Vs others\n",
    "print('Cohens d: %.3f' % cohend(MLPC_FMeasure, DT_FMeasure))    \n",
    "print('Cohens d: %.3f' % cohend(MLPC_FMeasure, MlkNN_FMeasure))    \n",
    "\n",
    "#Effect_Size MlkNN Vs others\n",
    "print('Cohens d: %.3f' % cohend(MlkNN_FMeasure,DT_FMeasure))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Wilcoxon paired and dependent samples\n",
    "\n",
    "#Fail to Reject H0: Sample distributions are equal.\n",
    "#Reject H0: Sample distributions are not equal.\n",
    "\n",
    "#from scipy.stats import wilcoxon\n",
    "\n",
    "#stat, p = wilcoxon(data1, data2)\n",
    "#print('Statistics=%.3f, p=%.3f' % (stat, p))\n",
    "# interpret\n",
    "#alpha = 0.05\n",
    "#if p > alpha:\n",
    "#\tprint('Same distribution (fail to reject H0)')\n",
    "#else:\n",
    "#\tprint('Different distribution (reject H0)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Precision analysis\n",
    "#Subsetting data to compute Stats\n",
    "\n",
    "Baseline_binary = dataBinary[dataBinary.Hypothesis == 'Baseline']\n",
    "Baseline_Precision = Baseline_binary['Precision']\n",
    "\n",
    "H5_binary = dataBinary[dataBinary.Hypothesis == 'H5']\n",
    "H5_Precision = H5_binary['Precision']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### MANN-U Independent Samples\n",
    "from scipy.stats import mannwhitneyu\n",
    "\n",
    "#Fail to Reject H0: Sample distributions are equal.\n",
    "#Reject H0: Sample distributions are not equal.\n",
    "\n",
    "##### Defining variables to be comparede\n",
    "\n",
    "# compare samples\n",
    "stat, p = mannwhitneyu(Baseline_Precision, H5_Precision)\n",
    "print('Statistics=%.3f, p=%.3f' % (stat, p))\n",
    "# interpret\n",
    "alpha = 0.05\n",
    "if p > alpha:\n",
    "\tprint('Same distribution (fail to reject H0)')\n",
    "else:\n",
    "\tprint('Different distribution (reject H0)')\n",
    "\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Effect_Size RandomForest Vs others   \n",
    "print(cliffsDelta(Baseline_Precision, H5_Precision))\n",
    "#print(cliffsDelta(RF_Precision, MLPC_Precision))\n",
    "#print(cliffsDelta(RF_Precision, DT_Precision))\n",
    "#print(cliffsDelta(RF_Precision, MlkNN_Precision)) #divisionByZero error\n",
    "\n",
    "#Effect_Size LogisticRegression Vs others\n",
    "#print(cliffsDelta(LogisticRegression_Precision, MLPC_Precision))\n",
    "#print(cliffsDelta(LogisticRegression_Precision, DT_Precision))\n",
    "#print(cliffsDelta(LogisticRegression_Precision, MlkNN_Precision))\n",
    "\n",
    "#Effect_Size MLPC Vs others\n",
    "#print(cliffsDelta(MLPC_Precision, DT_Precision))\n",
    "#print(cliffsDelta(MLPC_Precision, MlkNN_Precision))\n",
    "\n",
    "#Effect_Size MlkNN Vs others\n",
    "#print(cliffsDelta(MlkNN_Precision,DT_Precision))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Kruskall Wallis - Group Comparison\n",
    "\n",
    "#Fail to Reject H0: Paired sample distributions are equal.\n",
    "#Reject H0: Paired sample distributions are not equal.\n",
    "    \n",
    "from scipy.stats import kruskal\n",
    "# seed the random number generator\n",
    "\n",
    "\n",
    "# compare samples\n",
    "stat, p = kruskal(RF_Precision,LogisticRegression_Precision, MLPC_Precision,DT_Precision,MlkNN_Precision)\n",
    "print('Statistics=%.3f, p=%.3f' % (stat, p))\n",
    "# interpret\n",
    "alpha = 0.05\n",
    "if p > alpha:\n",
    "\tprint('Same distributions (fail to reject H0)')\n",
    "else:\n",
    "\tprint('Different distributions (reject H0)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://machinelearningmastery.com/nonparametric-statistical-significance-tests-in-python/\n",
    "from scipy.stats import friedmanchisquare\n",
    "\n",
    "# compare samples\n",
    "stat, p = friedmanchisquare(RF_Precision,LogisticRegression_Precision, MLPC_Precision,DT_Precision,MlkNN_Precision)\n",
    "print('Statistics=%.3f, p=%.3f' % (stat, p))\n",
    "# interpret\n",
    "alpha = 0.05\n",
    "if p > alpha:\n",
    "\tprint('Same distributions (fail to reject H0)')\n",
    "else:\n",
    "\tprint('Different distributions (reject H0)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
